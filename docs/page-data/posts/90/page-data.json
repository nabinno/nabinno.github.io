{"componentChunkName":"component---src-templates-post-tsx","path":"/posts/90","result":{"data":{"esaPost":{"number":90,"relative_category":"blog/backend","fields":{"title":"yubinbango-dataをどうやって生成するか","excerpt":"郵便番号から住所を補完するライブラリ「yubinbango」を継続的に利用するにあたり、当該ライブラリが参照している郵便データ「yubinbango-data」を自前でメンテナンスできるか確認します。   > PROBLEMPROBLEM \n\n- 「yubinbango/yubinbango」を利用するにあたり「yubinbango/yubinbango-data」の更新が継続的に行われるかサービス継続性の懸念がある そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい \n- そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい  > SOLUTIONSOLUTION \n\nというわけで、yubinbango-dataの中身であるken_all.csvとjigyosyo.csvを安定して変換する方法を確認します。  > ken_all.csvを正規化するken_all.csvを正規化する \n\nyubinbango-dataのken_all.csvの部分はアイビスが提供しているzipcloudを参照しているようなので、そちらに合わせて利用します。 sh\n\nsudo apt install nkf { curl -sSL \"http://zipcloud.ibsnet.co.jp/zipcodedata/download?di=1625040649647\" -o ./x_ken_all.zip; unzip -p x_ken_all.zip | nkf -w; rm x_ken_all.zip } >ken_all.csv  \n\nzipcloudを使うことに抵抗がある場合はgokenallもありますが、正規化によって一つの郵便番号に複数の町名番地が存在するため郵便番号をキーとするyubinbango-dataに変換する際には工夫が必要です。 sh\n\ngo get github.com/oirik/gokenall/cmd/kenall { kenall download -x | kenall normalize } >ken_all.csv   > jigyosyo.csvを取得するjigyosyo.csvを取得する \n\njigyosyo.csvは特に正規化は必要ないです。 sh\n\n{ curl -sSL https://www.post.japanpost.jp/zipcode/dl/jigyosyo/zip/jigyosyo.zip -o ./jigyosyo.zip; unzip -p jigyosyo.zip | nkf -w; rm jigyosyo.zip } >jigyosyo.csv   > yubinbango-dataを生成するyubinbango-dataを生成する \n\nken_all.csvとjigyosyo.csvをUNIONしてjqで郵便番号をキーとしたオブジェクトに変換します。一部buildingカラムがnullを持っておりyubinbango-dataと異なる部分はありますが、大凡同等の状態にまで持っていくことが出来ました。 sh\n\nbrew install noborus/tap/trdsql for i in {001..999}; do trdsql -ojson \" SELECT * FROM ( SELECT c3 zip, c8 city, c9 town, NULL building FROM ken_all.csv UNION SELECT c8 zip, c5 city, c6 town, c7 building FROM jigyosyo.csv ) WHERE SUBSTRING(zip,0,4) = '$i' ORDER BY zip ASC \" \\ | jq --compact-output ' . | to_entries | map({ (.value.zip): [1, .value.city, .value.town, .value.building] }) | add ' \\ | sed -E 's/(.+?)/$yubin(\\1);/g' \\ >$i.js; done   > WRAPUPWRAPUP \n\n昔から何かと話題に上がるken_all.csvですが、正規化のサービスに加えCSV用SQLクライアントとjqの登場により思った以上に簡単に変換することができました。","thumbnail":"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png"},"wip":false,"body_md":"郵便番号から住所を補完するライブラリ「yubinbango」を継続的に利用するにあたり、当該ライブラリが参照している郵便データ「yubinbango-data」を自前でメンテナンスできるか確認します。\r\n\r\n<img width=\"710\" alt=\"thumbnail\" src=\"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png\">\r\n\r\n# PROBLEM\r\n- 「[yubinbango/yubinbango](https://github.com/yubinbango/yubinbango)」を利用するにあたり「[yubinbango/yubinbango-data](https://github.com/yubinbango/yubinbango-data)」の更新が継続的に行われるかサービス継続性の懸念がある\r\n    - そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい\r\n\r\n# SOLUTION\r\nというわけで、yubinbango-dataの中身である[ken_all.csvとjigyosyo.csv](https://www.post.japanpost.jp/zipcode/download.html)を安定して変換する方法を確認します。\r\n\r\n## ken_all.csvを正規化する\r\nyubinbango-dataのken_all.csvの部分はアイビスが提供している[zipcloud](http://zipcloud.ibsnet.co.jp)を参照しているようなので、そちらに合わせて利用します。\r\n```sh\r\nsudo apt install nkf\r\n{ \r\n  curl -sSL \"http://zipcloud.ibsnet.co.jp/zipcodedata/download?di=1625040649647\" -o ./x_ken_all.zip;\r\n  unzip -p x_ken_all.zip | nkf -w;\r\n  rm x_ken_all.zip\r\n} >ken_all.csv\r\n```\r\n\r\nzipcloudを使うことに抵抗がある場合は[gokenall](https://github.com/oirik/gokenall)もありますが、正規化によって一つの郵便番号に複数の町名番地が存在するため郵便番号をキーとするyubinbango-dataに変換する際には工夫が必要です。\r\n\r\n```sh\r\ngo get github.com/oirik/gokenall/cmd/kenall\r\n{ kenall download -x | kenall normalize } >ken_all.csv\r\n```\r\n\r\n## jigyosyo.csvを取得する\r\njigyosyo.csvは特に正規化は必要ないです。\r\n\r\n```sh\r\n{ \r\n  curl -sSL https://www.post.japanpost.jp/zipcode/dl/jigyosyo/zip/jigyosyo.zip -o ./jigyosyo.zip;\r\n  unzip -p jigyosyo.zip | nkf -w;\r\n  rm jigyosyo.zip\r\n} >jigyosyo.csv\r\n```\r\n\r\n## yubinbango-dataを生成する\r\nken_all.csvとjigyosyo.csvをUNIONしてjqで郵便番号をキーとしたオブジェクトに変換します。一部buildingカラムがnullを持っておりyubinbango-dataと異なる部分はありますが、大凡同等の状態にまで持っていくことが出来ました。\r\n\r\n```sh\r\nbrew install noborus/tap/trdsql\r\nfor i in {001..999}; do\r\n  trdsql -ojson \"\r\n    SELECT *\r\n    FROM (\r\n      SELECT c3 zip, c8 city, c9 town, NULL building FROM ken_all.csv\r\n      UNION SELECT c8 zip, c5 city, c6 town, c7 building FROM jigyosyo.csv\r\n    )\r\n    WHERE SUBSTRING(zip,0,4) = '$i'\r\n    ORDER BY zip ASC\r\n  \" \\\r\n  | jq --compact-output '\r\n    .\r\n    | to_entries\r\n    | map({ (.value.zip): [1, .value.city, .value.town, .value.building] })\r\n    | add\r\n  ' \\\r\n  | sed -E 's/(.+?)/$yubin(\\1);/g' \\\r\n  >$i.js;\r\ndone\r\n```\r\n\r\n# WRAPUP\r\n昔から何かと話題に上がるken_all.csvですが、正規化のサービスに加えCSV用SQLクライアントとjqの登場により思った以上に簡単に変換することができました。","body_html":"<p data-sourcepos=\"1:1-1:250\">郵便番号から住所を補完するライブラリ「yubinbango」を継続的に利用するにあたり、当該ライブラリが参照している郵便データ「yubinbango-data」を自前でメンテナンスできるか確認します。</p>\n<a href=\"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png\" target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"710\" alt=\"thumbnail\" src=\"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png\"></a>\n<h1 data-sourcepos=\"5:1-5:9\" id=\"1-0-0\" name=\"1-0-0\"><a class=\"anchor\" id=\"PROBLEM\" name=\"PROBLEM\" href=\"#PROBLEM\" data-position=\"1-0-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"PROBLEM\"> &gt; PROBLEM</span></a>PROBLEM</h1>\n<ul data-sourcepos=\"6:1-8:0\">\n<li data-sourcepos=\"6:1-8:0\">「<a href=\"https://github.com/yubinbango/yubinbango\" target=\"_blank\" rel=\"noopener noreferrer\">yubinbango/yubinbango</a>」を利用するにあたり「<a href=\"https://github.com/yubinbango/yubinbango-data\" target=\"_blank\" rel=\"noopener noreferrer\">yubinbango/yubinbango-data</a>」の更新が継続的に行われるかサービス継続性の懸念がある\n<ul data-sourcepos=\"7:5-8:0\">\n<li data-sourcepos=\"7:5-8:0\">そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい</li>\n</ul></li>\n</ul>\n<h1 data-sourcepos=\"9:1-9:10\" id=\"2-0-0\" name=\"2-0-0\"><a class=\"anchor\" id=\"SOLUTION\" name=\"SOLUTION\" href=\"#SOLUTION\" data-position=\"2-0-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"SOLUTION\"> &gt; SOLUTION</span></a>SOLUTION</h1>\n<p data-sourcepos=\"10:1-10:189\">というわけで、yubinbango-dataの中身である<a href=\"https://www.post.japanpost.jp/zipcode/download.html\" target=\"_blank\" rel=\"noopener noreferrer\">ken_all.csvとjigyosyo.csv</a>を安定して変換する方法を確認します。</p>\n<h2 data-sourcepos=\"12:1-12:32\" id=\"2-1-0\" name=\"2-1-0\"><a class=\"anchor\" id=\"ken_all.csvを正規化する\" name=\"ken_all.csvを正規化する\" href=\"#ken_all.csvを正規化する\" data-position=\"2-1-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"ken_all.csvを正規化する\"> &gt; ken_all.csvを正規化する</span></a>ken_all.csvを正規化する</h2>\n<p data-sourcepos=\"13:1-13:195\">yubinbango-dataのken_all.csvの部分はアイビスが提供している<a href=\"http://zipcloud.ibsnet.co.jp\" target=\"_blank\" rel=\"noopener noreferrer\">zipcloud</a>を参照しているようなので、そちらに合わせて利用します。</p>\n<div class=\"code-block\" data-sourcepos=\"14:1-21:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code><span class=\"nb\">sudo </span>apt <span class=\"nb\">install </span>nkf\n<span class=\"o\">{</span> \n  curl <span class=\"nt\">-sSL</span> <span class=\"s2\">\"http://zipcloud.ibsnet.co.jp/zipcodedata/download?di=1625040649647\"</span> <span class=\"nt\">-o</span> ./x_ken_all.zip<span class=\"p\">;</span>\n  unzip <span class=\"nt\">-p</span> x_ken_all.zip | nkf <span class=\"nt\">-w</span><span class=\"p\">;</span>\n  <span class=\"nb\">rm </span>x_ken_all.zip\n<span class=\"o\">}</span> <span class=\"o\">&gt;</span>ken_all.csv\n</code></pre></div></div>\n<p data-sourcepos=\"23:1-23:296\">zipcloudを使うことに抵抗がある場合は<a href=\"https://github.com/oirik/gokenall\" target=\"_blank\" rel=\"noopener noreferrer\">gokenall</a>もありますが、正規化によって一つの郵便番号に複数の町名番地が存在するため郵便番号をキーとするyubinbango-dataに変換する際には工夫が必要です。</p>\n<div class=\"code-block\" data-sourcepos=\"25:1-28:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code>go get github.com/oirik/gokenall/cmd/kenall\n<span class=\"o\">{</span> kenall download <span class=\"nt\">-x</span> | kenall normalize <span class=\"o\">}</span> <span class=\"o\">&gt;</span>ken_all.csv\n</code></pre></div></div>\n<h2 data-sourcepos=\"30:1-30:30\" id=\"2-2-0\" name=\"2-2-0\"><a class=\"anchor\" id=\"jigyosyo.csvを取得する\" name=\"jigyosyo.csvを取得する\" href=\"#jigyosyo.csvを取得する\" data-position=\"2-2-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"jigyosyo.csvを取得する\"> &gt; jigyosyo.csvを取得する</span></a>jigyosyo.csvを取得する</h2>\n<p data-sourcepos=\"31:1-31:54\">jigyosyo.csvは特に正規化は必要ないです。</p>\n<div class=\"code-block\" data-sourcepos=\"33:1-39:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code><span class=\"o\">{</span> \n  curl <span class=\"nt\">-sSL</span> https://www.post.japanpost.jp/zipcode/dl/jigyosyo/zip/jigyosyo.zip <span class=\"nt\">-o</span> ./jigyosyo.zip<span class=\"p\">;</span>\n  unzip <span class=\"nt\">-p</span> jigyosyo.zip | nkf <span class=\"nt\">-w</span><span class=\"p\">;</span>\n  <span class=\"nb\">rm </span>jigyosyo.zip\n<span class=\"o\">}</span> <span class=\"o\">&gt;</span>jigyosyo.csv\n</code></pre></div></div>\n<h2 data-sourcepos=\"41:1-41:33\" id=\"2-3-0\" name=\"2-3-0\"><a class=\"anchor\" id=\"yubinbango-dataを生成する\" name=\"yubinbango-dataを生成する\" href=\"#yubinbango-dataを生成する\" data-position=\"2-3-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"yubinbango-dataを生成する\"> &gt; yubinbango-dataを生成する</span></a>yubinbango-dataを生成する</h2>\n<p data-sourcepos=\"42:1-42:288\">ken_all.csvとjigyosyo.csvをUNIONしてjqで郵便番号をキーとしたオブジェクトに変換します。一部buildingカラムがnullを持っておりyubinbango-dataと異なる部分はありますが、大凡同等の状態にまで持っていくことが出来ました。</p>\n<div class=\"code-block\" data-sourcepos=\"44:1-65:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code>brew <span class=\"nb\">install </span>noborus/tap/trdsql\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>001..999<span class=\"o\">}</span><span class=\"p\">;</span> <span class=\"k\">do\n  </span>trdsql <span class=\"nt\">-ojson</span> <span class=\"s2\">\"\n    SELECT *\n    FROM (\n      SELECT c3 zip, c8 city, c9 town, NULL building FROM ken_all.csv\n      UNION SELECT c8 zip, c5 city, c6 town, c7 building FROM jigyosyo.csv\n    )\n    WHERE SUBSTRING(zip,0,4) = '</span><span class=\"nv\">$i</span><span class=\"s2\">'\n    ORDER BY zip ASC\n  \"</span> <span class=\"se\">\\</span>\n  | jq <span class=\"nt\">--compact-output</span> <span class=\"s1\">'\n    .\n    | to_entries\n    | map({ (.value.zip): [1, .value.city, .value.town, .value.building] })\n    | add\n  '</span> <span class=\"se\">\\</span>\n  | <span class=\"nb\">sed</span> <span class=\"nt\">-E</span> <span class=\"s1\">'s/(.+?)/$yubin(\\1);/g'</span> <span class=\"se\">\\</span>\n  <span class=\"o\">&gt;</span><span class=\"nv\">$i</span>.js<span class=\"p\">;</span>\n<span class=\"k\">done</span>\n</code></pre></div></div>\n<h1 data-sourcepos=\"67:1-67:8\" id=\"3-0-0\" name=\"3-0-0\"><a class=\"anchor\" id=\"WRAPUP\" name=\"WRAPUP\" href=\"#WRAPUP\" data-position=\"3-0-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"WRAPUP\"> &gt; WRAPUP</span></a>WRAPUP</h1>\n<p data-sourcepos=\"68:1-68:208\">昔から何かと話題に上がるken_all.csvですが、正規化のサービスに加えCSV用SQLクライアントとjqの登場により思った以上に簡単に変換することができました。</p>\n","tags":["yubinbango","ken_all.csv","jq","trdsql"],"updated_at":"2021-07-25T12:07:20+09:00","childPublishedDate":{"published_on":"2021-07-25T00:00:00.000Z"},"updated_by":{"name":"なびの👷","screen_name":"nabinno","icon":"https://img.esa.io/uploads/production/members/94286/icon/thumb_m_ef5f024307008aa399b91f87fa5f64e8.jpg"}},"relatedPosts":{"edges":[{"node":{"number":139,"relative_category":"blog/backend","fields":{"title":"ヘルステック界隈のエンジニアが気をつけるべき個人情報の扱い","excerpt":"ヘルステックでエンジニアをしている方であればデータの扱いには苦労していることと思います。CISOがつくったデータセグメンテーションがどういう意図で成り立っているのか、整理されていない現場だと読み解きに時間がかかります。現場に入って早々 何も知らないエンジニアとしては、緩めな方針よりは保守的に設計していく方が後々のトラブルが少なく安全です。   > PROBLEMPROBLEM \n\n- 要配慮個人情報について、厚労省医政局発「医療情報システムの安全管理に関するガイドライン」1を見ると「 医療・健康情報を[..]医師等以外の者が分析等を実施することは許されるものではない 」と書かれている ここでいう「 医療・健康情報 」は要配慮個人情報の中の具体的に何を指しているのか分かりづらい 「 医師等 」の「 等 」が何を指すのか分かりづらい 厚労省医政局の発令0912001号「診療情報の提供等に関する指針」2から推察するに、「 医療・健康情報 」は診療録、「 医師等 」は医療系有資格者を指している 医療系有資格者については、個人情報保護法の関連で出された医療・介護分野用「医療・介護関係事業者における個人情報の適切な取扱いのためのガイダンス」に掲載されている守秘義務対象 なお、国家資格を有しないパラメディカル等はグレーゾーンになっている \n- ここでいう「 医療・健康情報 」は要配慮個人情報の中の具体的に何を指しているのか分かりづらい\n- 「 医師等 」の「 等 」が何を指すのか分かりづらい\n- 厚労省医政局の発令0912001号「診療情報の提供等に関する指針」2から推察するに、「 医療・健康情報 」は診療録、「 医師等 」は医療系有資格者を指している 医療系有資格者については、個人情報保護法の関連で出された医療・介護分野用「医療・介護関係事業者における個人情報の適切な取扱いのためのガイダンス」に掲載されている守秘義務対象 なお、国家資格を有しないパラメディカル等はグレーゾーンになっている \n- 医療系有資格者については、個人情報保護法の関連で出された医療・介護分野用「医療・介護関係事業者における個人情報の適切な取扱いのためのガイダンス」に掲載されている守秘義務対象 なお、国家資格を有しないパラメディカル等はグレーゾーンになっている \n- なお、国家資格を有しないパラメディカル等はグレーゾーンになっている\n- また、データアクセス対象を緩めると、教育が不十分な人が故意に流出させ刑法上の秘密漏示罪3に問われる可能性がある 秘密漏示罪は身分犯ではあるが歯科医師のように解釈の余地もあり範囲が不透明 \n- 秘密漏示罪は身分犯ではあるが歯科医師のように解釈の余地もあり範囲が不透明  > SOLUTIONSOLUTION \n\nというわけで、ヘルステックに関わる個人情報の扱いを整理してみました。 \n\n課題は上記の通りで、時代の流れとともに医療情報の整備が進んでいる状況です。善管注意の責務を負ったエンジニアとしては医療系有資格者以外への診療録の情報提供は、例え、同僚であっても連結可能匿名（仮名加工）ではなく匿名加工で対応すべきでしょう。ゆくゆくは会社として次世代医療基盤法を適用し、医療分野の研究開発に資するよう体制を構築することが望ましいと考えています。  > 加工なし加工なし \n\n学術研究をのぞき第三者提供は本人同意が必要となるため、ユースケースは限定されます。各々の個人情報の種類によりアクセス出来る人が変わってきます。また、守秘義務が課せらる範囲が広く、行為によっては秘密漏示罪や不正アクセス禁止法4の罰則の対象になります。     診療録 診療録を除いた要配慮個人情報 要配慮個人情報を除いた個人情報     使用場所 社内 (医療関連有資格者) 社内 社内, 社外   利用目的の必要性 (公表有無) 必要 必要 必要   利用目的の必要性 (変更可否) 関連性を有する合理的な範囲 関連性を有する合理的な範囲 関連性を有する合理的な範囲   目的外利用 不可 不可 不可   第三者提供 (可否) 可 可 可   第三者提供 (本人同意) 必要 (オプトインのみ) 必要 (オプトインのみ) 必要 (オプトアウト)   個人の開示請求 応じる 応じる 応じる   漏洩時の報告 必須 必須 必須     > 仮名加工仮名加工 \n\n診療録に関する規定は次世代医療基盤法でまとめられているので、あえて規定が曖昧な仮名加工（連結可能匿名）をつかうのは望ましくありません。ユースケースとして要配慮個人情報を除いた個人情報の統計分析に限られるでしょう。     診療録を除いた要配慮個人情報 要配慮個人情報を除いた個人情報     使用場所 社内 社内   利用目的の必要性 (公表有無) 必要 必要   利用目的の必要性 (変更可否) 際限なく変更可能 際限なく変更可能   目的外利用 不可 不可   第三者提供 (可否) 不可 不可   個人の開示請求 応じない 応じない   漏洩時の報告 なし なし     > 匿名加工匿名加工 \n\n診療録は本人のオプトアウトありですが、基本本人同意なしで利用可能です。ただ、診療録は可変長文字列の上、特異な記述として最も気をつける対象になります。データマスキングの実装は手厚く行っていく必要があります。     診療録を含んだ要配慮個人情報 要配慮個人情報を除いた個人情報     使用場所 社内, 社外 社内, 社外   利用目的の必要性 (公表有無) 不要 不要   第三者提供 (可否) 可 可   第三者提供 (本人同意) 不要 (オプトアウトあり) 不要   個人の開示請求 応じない 応じない   漏洩時の報告 なし なし     > WRAPUPWRAPUP \n\nポイントをかいつまんでまとめてみました。エンジニア視点のため、考慮漏れの箇所があるかも知れませんが、フィードバックや各種レギュレーションの経過を元に更新していければと思います。  \n\n1. https://www.mhlw.go.jp/stf/shingi/0000516275.html ↩ \n2. https://www.mhlw.go.jp/web/t_doc?dataId=00tb3403&dataType=1&page%20No=1 ↩ \n3. https://elaws.e-gov.go.jp/document?lawid=140AC0000000045 ↩ \n4. https://elaws.e-gov.go.jp/document?lawid=411AC0000000128 ↩"},"name":"[2022-04-24]ヘルステック界隈のエンジニアが気をつけるべき個人情報の扱い","tags":["privacy","data-masking","data-engineering","health-informatics"],"childPublishedDate":{"published_on":"2022-04-24T00:00:00.000Z","published_on_unix":1650758400}}},{"node":{"number":137,"relative_category":"blog/backend","fields":{"title":"G Suite無償版停止に伴い、MXレコード等のドメイン管理を整理した","excerpt":"今回は10年以上利用していたG Suite無償版が2022年8月に停止されるとのことで、メールアドレスの管理をどうするか検討しました。メール管理は別のGMailアカウントを使っていたので、転送できれば良いのですが、これを機にドメイン管理をAWSにまとめていくことを思いつきました。   > PROBLEMPROBLEM \n\n- 10年以上利用していたG Suite無償版が2022年8月に停止され、メールアドレスの管理をどうしようか Google Workspaceにアップグレードしても良いけどBusiness Starterプランにしても面白みがないので別の方法を探したい G Suiteはメールしか利用しておらず、メール管理は別のGMailアカウントを使っていたのでメール転送機能で代替できそう \n- Google Workspaceにアップグレードしても良いけどBusiness Starterプランにしても面白みがないので別の方法を探したい G Suiteはメールしか利用しておらず、メール管理は別のGMailアカウントを使っていたのでメール転送機能で代替できそう \n- G Suiteはメールしか利用しておらず、メール管理は別のGMailアカウントを使っていたのでメール転送機能で代替できそう  > SOLUTIONSOLUTION \n\nと言うわけで、今回はG Suiteアカウントの利用を止めて、MXレコード周りを整理することにしました。個人利用なのでドメイン管理は既存のままで良かったのですが、証跡管理のない状況に耐えられずAWSに移管。メール転送機能はPOBOX以外はサブアドレス対応していなかったのですが、キャッチオール対応できるのでまずは良しとしています。現時点での構成は下記の通り。 \n\nなお、複数人数で必要になった場合は、サブアドレスとグループアドレスが対応可能なAmazon Workmailに移管する予定ですが、これでもGoogle Workspaceを利用するよりコストは半分程で済みます。  > 構成構成  > beforebefore \n\n- ドメイン管理 バリュードメイン\n- NSレコード Cloudflare DNS\n- MXレコード G Suite\n- SMTP G Suite  > afterafter \n\n- ドメイン管理 Amazon Route 53\n- NSレコード Amazon Route 53\n- MXレコード Cloudflare Email Routing\n- SMTP Amazon SES  > 手順手順 \n\n方針が決まるまでいくつかメールサービスを検討したのですが、決まってしまえばやることは単純です。  > 1. ドメイン管理を整理する1. ドメイン管理を整理する \n\n基本はドメイン移管申請ですが、G Suiteを後ほど削除することを考慮してMXレコードをCloudflare Email Routingに変更。本来はこの処理の前にG Suiteに紐付いている各サービスの設定変更が必要になります。 \n\n1. 移管元にてWHOIS情報公開代行の解除\n2. 移管元にてドメインロックの解除\n3. 移管元にて認証鍵 (Auth-Code) を確認\n4. 移管先にてホストゾーンの作成、各レコードの内容を移管元に合わせる\n5. Cloudflare Email Routingにて転送先メールアドレスを検証する\n6. 移管先にてMXレコードをCloudflare Email Routingのものを設定する\n7. 移管元のNSレコードを移管先に変更\n8. 移管先にて認証鍵をつかい移管申請を行う\n9. 移管元に対して移管申請を行った旨をメールにて連携する \n\nCf. \n\n- ドメインの他社への移管 | バリュードメイン ユーザーガイド\n- ドメイン登録の Amazon Route 53 への移管 - Amazon Route 53\n- Easily creating and routing email addresses with Cloudflare Email Routing  > 2. SMTPを設定する2. SMTPを設定する \n\n最近はセキュリティ対策のためGMailのSMTPが使いづらくなっているので、今回はAmazon SESを利用しました。サンドボックス解除のため下記の通りサポートに依頼しました。 txt\n\n# メールタイプ 通例の取引がメインとなる予定です # ユースケース ## メールを送信する頻度 週に1-2回 ## 受信者リストのメンテナンス方法 四半期に一度の棚卸し ## バウンス対応 当該メールアドレスの削除 ## 申し立て対応 当該メールアドレスへのフラグ管理 ## 解除申請の管理方法 メールでの受付 ## 送信予定のメールサンプル {{宛先名}}様 お世話になっております。 表題の件につきまして1点問い合わせします。 {{問い合わせ内容}} ご不明な点等ございましたらお気軽にお問い合わせ下さい。 どうぞ、よろしくお願い致します。   > 3. G Suiteを退会する3. G Suiteを退会する \n\nG Suiteに依存しているサービスがないか確認し、退会します。  > WRAPUPWRAPUP \n\n以前からドメイン管理をAWSに移管したかったのですが、積極的な理由がないためなおざりになっていました。今回のG Suite無償版の期限切れに伴い整理できすっきりしたので、これを機にいろいろ整理していきたいですね。  > 後日談後日談 \n\nCloudflare DNSからAmazon Route 53に設定を変更した数日後、Cloudflare Email Routingが使用できなくなりました。転送機能としてドメイン管理から切り離されていると思ったのですがそうではなかったようです。ドメイン管理は厳しめに証跡を取っていきたいところなので、Amazon Route 53による管理は譲れません。 \n\nまた、Amazon SESによる転送機能も検討したのですが、送信元すべてに対してドメイン検証が必要なため現実的ではありませんでした。AWSにはメール転送の種類が2つあって、送信元を転送者に置き換える「転送」と送信元をそのまま利用する「リダイレクト」があります。「転送」だと元の送信元とコミュニケーションが取りづらくなる一方、「リダイレクト」だとすべての送信元のドメイン検証が必要となります。ここでは融通が利かないと判断するのではなく、セキュリティを考慮された実装と捉え、AWSが提供しているWorkMailを素直に使うことにしました。慣れればたいしたことはありません。下記が結果になります。  > beforebefore \n\n- ドメイン管理 Amazon Route 53\n- NSレコード Amazon Route 53\n- MXレコード Cloudflare Email Routing\n- SMTP Amazon SES  > afterafter \n\n- ドメイン管理 Amazon Route 53\n- NSレコード Amazon Route 53\n- MXレコード Amazon WorkMail\n- SMTP Amazon WorkMail"},"name":"[2022-04-12]G Suite無償版停止に伴い、MXレコード等のドメイン管理を整理した","tags":["gsuite","google-workspace","cloudflare","amazon-ses","amazon-workmail"],"childPublishedDate":{"published_on":"2022-04-12T00:00:00.000Z","published_on_unix":1649721600}}},{"node":{"number":119,"relative_category":"blog/backend","fields":{"title":"踏み台をSSM Session ManagerとAWS SSOで実現する","excerpt":"踏み台のユーザーが増えてきたため公開鍵管理や監視と運用負荷が上がってきました。オペミスが発生しやすい上 監査的な意味で無視できない状況になってきたので重い腰を上げることにしました。   > PROBLEMPROBLEM \n\n- EC2インスタンスの踏み台運用がつらい 公開鍵管理がつらい 提出・設定・確認ともに運用コストがかかる AWSアカウント数 x ユーザー数で指数関数的に運用コストが増していくことが想定される インフラ管理が分散していると、提出側・設定側ともに重複コストが発生する 監視運用がつらい 定期的に踏み台がブルートフォース攻撃を受けており、脅威が低いとは言えストレスがかかる 踏み台アクセスへの監査ログが不十分 \n- 公開鍵管理がつらい 提出・設定・確認ともに運用コストがかかる AWSアカウント数 x ユーザー数で指数関数的に運用コストが増していくことが想定される インフラ管理が分散していると、提出側・設定側ともに重複コストが発生する \n- 提出・設定・確認ともに運用コストがかかる\n- AWSアカウント数 x ユーザー数で指数関数的に運用コストが増していくことが想定される\n- インフラ管理が分散していると、提出側・設定側ともに重複コストが発生する\n- 監視運用がつらい 定期的に踏み台がブルートフォース攻撃を受けており、脅威が低いとは言えストレスがかかる 踏み台アクセスへの監査ログが不十分 \n- 定期的に踏み台がブルートフォース攻撃を受けており、脅威が低いとは言えストレスがかかる\n- 踏み台アクセスへの監査ログが不十分  > SOLUTIONSOLUTION \n\nというわけで、Session ManagerとSSOでアクセス管理の効率化を狙います。  > 踏み台サーバーの設定踏み台サーバーの設定 \n\nまず、データフローとしては下記の図の通りで、今回はプライベートサブネット上にEC2を置いて素のSession ManagerでDBへの接続することにします。当該インスタンスは AmazonSSMManagedInstanceCore ポリシー1を含んだロールを適用。なお、ECS ExecではSession Managerでポートフォワーディングを実現でき無かったことに加え、既存の踏み台資産を流用するため今回の実装対象から外しました。 \n\n  > SSOの設定SSOの設定 \n\n踏み台サーバーの設定が終わったら、次に当該インスタンスへ接続するためにSSOで渡すロールをアクセス権限セットに設定します。下記カスタムポリシーはEC2インスタンスにアクセスするための必要最低限のものになります。 カスタムポリシー json\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\", \"ds:CreateComputer\", \"ds:DescribeDirectories\", \"ec2:DescribeInstanceStatus\", \"logs:*\", \"ssm:*\", \"ec2messages:*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [ \"arn:aws:ssm:*:*:session/<EC2インスタンスID>\", \"arn:aws:ec2:*:*:instance/<EC2インスタンスID>\" ] }, { \"Effect\": \"Deny\", \"Action\": [ \"ssm:Describe*\", \"ssm:Get*\", \"ssm:List*\", \"logs:Describe*\", \"logs:Get*\", \"logs:List*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"iam:CreateServiceLinkedRole\", \"Resource\": \"arn:aws:iam::*:role/aws-service-role/ssm.amazonaws.com/AWSServiceRoleForAmazonSSM*\", \"Condition\": { \"StringLike\": { \"iam:AWSServiceName\": \"ssm.amazonaws.com\" } } }, { \"Effect\": \"Allow\", \"Action\": \"iam:CreateServiceLinkedRole\", \"Resource\": \"arn:aws:iam::*:role/aws-service-role/ssm.amazonaws.com/AWSServiceRoleForAmazonSSM*\", \"Condition\": { \"StringLike\": { \"iam:AWSServiceName\": \"ssm.amazonaws.com\" } } }, { \"Effect\": \"Allow\", \"Action\": [ \"iam:DeleteServiceLinkedRole\", \"iam:GetServiceLinkedRoleDeletionStatus\" ], \"Resource\": \"arn:aws:iam::*:role/aws-service-role/ssm.amazonaws.com/AWSServiceRoleForAmazonSSM*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ssmmessages:CreateControlChannel\", \"ssmmessages:CreateDataChannel\", \"ssmmessages:OpenControlChannel\", \"ssmmessages:OpenDataChannel\" ], \"Resource\": \"*\" } ] }    > セッションを張るための事前準備セッションを張るための事前準備 \n\nセッションを張るためには下記3つの手順が必要になります。SSO経由のセッション設定が2通りありますが、クレデンシャル方式はセッションが切れる毎に変更する手間があるため、CLI方式をお薦めします。 \n\n1. AWS CLI v2をインストール\n2. 下記いずれかの方式でSSO経由のセッション設定を行う クレデンシャル方式 CLI（ aws sso login ）方式 \n3. クレデンシャル方式\n4. CLI（ aws sso login ）方式\n5. Session Manager プラグインをインストール  > DBクライアントの設定DBクライアントの設定 \n\n最後に、DBクライアントについて3つの手順を踏んで接続を試みます2。なお、ローカル環境でポートフォワーディングを都度行うのを省略したい方は、DataGripを利用すると良いでしょう。 \n\n1. ローカル環境にて ~/.ssh/config ファイルを編集 Session Managerにproxyと対象RDSのエンドポイントを記載 configHost <任意のhost名> HostName <※ 指定しなければlocalhostになる> User ec2-user ProxyCommand sh -c \"aws ssm start-session --target <接続する踏み台のインスタンスID> --document-name AWS-StartSSHSession --parameters 'portNumber=%p' --region ap-northeast-1 --profile <プロフィール>\" LocalForward <任意のポート> <RDSエンドポイント>:<RDSポート> IdentityFile ~/.ssh/<EC2に接続する秘密鍵> 設定したhost名でセッションマネージャー越しにssh接続できるかを確認 sh$ ssh <設定したhost名> \n2. Session Managerにproxyと対象RDSのエンドポイントを記載 configHost <任意のhost名> HostName <※ 指定しなければlocalhostになる> User ec2-user ProxyCommand sh -c \"aws ssm start-session --target <接続する踏み台のインスタンスID> --document-name AWS-StartSSHSession --parameters 'portNumber=%p' --region ap-northeast-1 --profile <プロフィール>\" LocalForward <任意のポート> <RDSエンドポイント>:<RDSポート> IdentityFile ~/.ssh/<EC2に接続する秘密鍵> \n3. 設定したhost名でセッションマネージャー越しにssh接続できるかを確認 sh$ ssh <設定したhost名> \n4. 手順1で設定したsshで接続することでポートフォワーディング\n5. DBクライアントで下記のように接続情報を設定し接続する Host: <手順1のconfigファイルにて任意指定したホスト名> Port: <手順4のconfigファイルにて任意指定したポート> 他項目: DB接続情報 \n6. Host: <手順1のconfigファイルにて任意指定したホスト名>\n7. Port: <手順4のconfigファイルにて任意指定したポート>\n8. 他項目: DB接続情報  > WRAPUPWRAPUP \n\nパブリックサブネット上の踏み台に慣れている方は馴染みのない方法に戸惑うかも知れませんが、踏み台資産を流用できるという意味で導入のコストもそれほどかかりませんし、ユーザーとしても利用の敷居は高くありませんでした。後々の管理コストを心配している方は一度検討してみてはいかがでしょうか。  \n\n1. AmazonEC2RoleforSSM は非推奨のため適用しないように注意します。 ↩ \n2. 今回はメンテナンスコストを避けるためSSH over SSMの関連ツール ssh-ssm.sh ssm-tool は使わない方針でいます。 ↩"},"name":"[2021-11-21]踏み台をSSM Session ManagerとAWS SSOで実現する","tags":["session-manager","aws-sso"],"childPublishedDate":{"published_on":"2021-11-21T00:00:00.000Z","published_on_unix":1637452800}}}]}},"pageContext":{"number":90}},"staticQueryHashes":[]}