{"componentChunkName":"component---src-templates-post-tsx","path":"/posts/90","result":{"data":{"esaPost":{"number":90,"relative_category":"blog/backend","fields":{"title":"yubinbango-dataをどうやって生成するか","excerpt":"郵便番号から住所を補完するライブラリ「yubinbango」を継続的に利用するにあたり、当該ライブラリが参照している郵便データ「yubinbango-data」を自前でメンテナンスできるか確認します。   > PROBLEMPROBLEM \n\n- 「yubinbango/yubinbango」を利用するにあたり「yubinbango/yubinbango-data」の更新が継続的に行われるかサービス継続性の懸念がある そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい \n- そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい  > SOLUTIONSOLUTION \n\nというわけで、yubinbango-dataの中身であるken_all.csvとjigyosyo.csvを安定して変換する方法を確認します。  > ken_all.csvを正規化するken_all.csvを正規化する \n\nyubinbango-dataのken_all.csvの部分はアイビスが提供しているzipcloudを参照しているようなので、そちらに合わせて利用します。 sh\n\nsudo apt install nkf { curl -sSL \"http://zipcloud.ibsnet.co.jp/zipcodedata/download?di=1625040649647\" -o ./x_ken_all.zip; unzip -p x_ken_all.zip | nkf -w; rm x_ken_all.zip } >ken_all.csv  \n\nzipcloudを使うことに抵抗がある場合はgokenallもありますが、正規化によって一つの郵便番号に複数の町名番地が存在するため郵便番号をキーとするyubinbango-dataに変換する際には工夫が必要です。 sh\n\ngo get github.com/oirik/gokenall/cmd/kenall { kenall download -x | kenall normalize } >ken_all.csv   > jigyosyo.csvを取得するjigyosyo.csvを取得する \n\njigyosyo.csvは特に正規化は必要ないです。 sh\n\n{ curl -sSL https://www.post.japanpost.jp/zipcode/dl/jigyosyo/zip/jigyosyo.zip -o ./jigyosyo.zip; unzip -p jigyosyo.zip | nkf -w; rm jigyosyo.zip } >jigyosyo.csv   > yubinbango-dataを生成するyubinbango-dataを生成する \n\nken_all.csvとjigyosyo.csvをUNIONしてjqで郵便番号をキーとしたオブジェクトに変換します。一部buildingカラムがnullを持っておりyubinbango-dataと異なる部分はありますが、大凡同等の状態にまで持っていくことが出来ました。 sh\n\nbrew install noborus/tap/trdsql for i in {001..999}; do trdsql -ojson \" SELECT * FROM ( SELECT c3 zip, c8 city, c9 town, NULL building FROM ken_all.csv UNION SELECT c8 zip, c5 city, c6 town, c7 building FROM jigyosyo.csv ) WHERE SUBSTRING(zip,0,4) = '$i' ORDER BY zip ASC \" \\ | jq --compact-output ' . | to_entries | map({ (.value.zip): [1, .value.city, .value.town, .value.building] }) | add ' \\ | sed -E 's/(.+?)/$yubin(\\1);/g' \\ >$i.js; done   > WRAPUPWRAPUP \n\n昔から何かと話題に上がるken_all.csvですが、正規化のサービスに加えCSV用SQLクライアントとjqの登場により思った以上に簡単に変換することができました。","thumbnail":"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png"},"wip":false,"body_md":"郵便番号から住所を補完するライブラリ「yubinbango」を継続的に利用するにあたり、当該ライブラリが参照している郵便データ「yubinbango-data」を自前でメンテナンスできるか確認します。\r\n\r\n<img width=\"710\" alt=\"thumbnail\" src=\"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png\">\r\n\r\n# PROBLEM\r\n- 「[yubinbango/yubinbango](https://github.com/yubinbango/yubinbango)」を利用するにあたり「[yubinbango/yubinbango-data](https://github.com/yubinbango/yubinbango-data)」の更新が継続的に行われるかサービス継続性の懸念がある\r\n    - そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい\r\n\r\n# SOLUTION\r\nというわけで、yubinbango-dataの中身である[ken_all.csvとjigyosyo.csv](https://www.post.japanpost.jp/zipcode/download.html)を安定して変換する方法を確認します。\r\n\r\n## ken_all.csvを正規化する\r\nyubinbango-dataのken_all.csvの部分はアイビスが提供している[zipcloud](http://zipcloud.ibsnet.co.jp)を参照しているようなので、そちらに合わせて利用します。\r\n```sh\r\nsudo apt install nkf\r\n{ \r\n  curl -sSL \"http://zipcloud.ibsnet.co.jp/zipcodedata/download?di=1625040649647\" -o ./x_ken_all.zip;\r\n  unzip -p x_ken_all.zip | nkf -w;\r\n  rm x_ken_all.zip\r\n} >ken_all.csv\r\n```\r\n\r\nzipcloudを使うことに抵抗がある場合は[gokenall](https://github.com/oirik/gokenall)もありますが、正規化によって一つの郵便番号に複数の町名番地が存在するため郵便番号をキーとするyubinbango-dataに変換する際には工夫が必要です。\r\n\r\n```sh\r\ngo get github.com/oirik/gokenall/cmd/kenall\r\n{ kenall download -x | kenall normalize } >ken_all.csv\r\n```\r\n\r\n## jigyosyo.csvを取得する\r\njigyosyo.csvは特に正規化は必要ないです。\r\n\r\n```sh\r\n{ \r\n  curl -sSL https://www.post.japanpost.jp/zipcode/dl/jigyosyo/zip/jigyosyo.zip -o ./jigyosyo.zip;\r\n  unzip -p jigyosyo.zip | nkf -w;\r\n  rm jigyosyo.zip\r\n} >jigyosyo.csv\r\n```\r\n\r\n## yubinbango-dataを生成する\r\nken_all.csvとjigyosyo.csvをUNIONしてjqで郵便番号をキーとしたオブジェクトに変換します。一部buildingカラムがnullを持っておりyubinbango-dataと異なる部分はありますが、大凡同等の状態にまで持っていくことが出来ました。\r\n\r\n```sh\r\nbrew install noborus/tap/trdsql\r\nfor i in {001..999}; do\r\n  trdsql -ojson \"\r\n    SELECT *\r\n    FROM (\r\n      SELECT c3 zip, c8 city, c9 town, NULL building FROM ken_all.csv\r\n      UNION SELECT c8 zip, c5 city, c6 town, c7 building FROM jigyosyo.csv\r\n    )\r\n    WHERE SUBSTRING(zip,0,4) = '$i'\r\n    ORDER BY zip ASC\r\n  \" \\\r\n  | jq --compact-output '\r\n    .\r\n    | to_entries\r\n    | map({ (.value.zip): [1, .value.city, .value.town, .value.building] })\r\n    | add\r\n  ' \\\r\n  | sed -E 's/(.+?)/$yubin(\\1);/g' \\\r\n  >$i.js;\r\ndone\r\n```\r\n\r\n# WRAPUP\r\n昔から何かと話題に上がるken_all.csvですが、正規化のサービスに加えCSV用SQLクライアントとjqの登場により思った以上に簡単に変換することができました。","body_html":"<p data-sourcepos=\"1:1-1:250\">郵便番号から住所を補完するライブラリ「yubinbango」を継続的に利用するにあたり、当該ライブラリが参照している郵便データ「yubinbango-data」を自前でメンテナンスできるか確認します。</p>\n<a href=\"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png\" target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"710\" alt=\"thumbnail\" src=\"https://img.esa.io/uploads/production/attachments/16651/2021/07/25/97367/d69e2c83-0aae-4409-804a-7f7ee0ce456c.png\"></a>\n<h1 data-sourcepos=\"5:1-5:9\" id=\"1-0-0\" name=\"1-0-0\"><a class=\"anchor\" id=\"PROBLEM\" name=\"PROBLEM\" href=\"#PROBLEM\" data-position=\"1-0-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"PROBLEM\"> &gt; PROBLEM</span></a>PROBLEM</h1>\n<ul data-sourcepos=\"6:1-8:0\">\n<li data-sourcepos=\"6:1-8:0\">「<a href=\"https://github.com/yubinbango/yubinbango\" target=\"_blank\" rel=\"noopener noreferrer\">yubinbango/yubinbango</a>」を利用するにあたり「<a href=\"https://github.com/yubinbango/yubinbango-data\" target=\"_blank\" rel=\"noopener noreferrer\">yubinbango/yubinbango-data</a>」の更新が継続的に行われるかサービス継続性の懸念がある\n<ul data-sourcepos=\"7:5-8:0\">\n<li data-sourcepos=\"7:5-8:0\">そこで自前でメンテナンスをする場合の対処方法を事前に知っておきたい</li>\n</ul></li>\n</ul>\n<h1 data-sourcepos=\"9:1-9:10\" id=\"2-0-0\" name=\"2-0-0\"><a class=\"anchor\" id=\"SOLUTION\" name=\"SOLUTION\" href=\"#SOLUTION\" data-position=\"2-0-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"SOLUTION\"> &gt; SOLUTION</span></a>SOLUTION</h1>\n<p data-sourcepos=\"10:1-10:189\">というわけで、yubinbango-dataの中身である<a href=\"https://www.post.japanpost.jp/zipcode/download.html\" target=\"_blank\" rel=\"noopener noreferrer\">ken_all.csvとjigyosyo.csv</a>を安定して変換する方法を確認します。</p>\n<h2 data-sourcepos=\"12:1-12:32\" id=\"2-1-0\" name=\"2-1-0\"><a class=\"anchor\" id=\"ken_all.csvを正規化する\" name=\"ken_all.csvを正規化する\" href=\"#ken_all.csvを正規化する\" data-position=\"2-1-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"ken_all.csvを正規化する\"> &gt; ken_all.csvを正規化する</span></a>ken_all.csvを正規化する</h2>\n<p data-sourcepos=\"13:1-13:195\">yubinbango-dataのken_all.csvの部分はアイビスが提供している<a href=\"http://zipcloud.ibsnet.co.jp\" target=\"_blank\" rel=\"noopener noreferrer\">zipcloud</a>を参照しているようなので、そちらに合わせて利用します。</p>\n<div class=\"code-block\" data-sourcepos=\"14:1-21:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code><span class=\"nb\">sudo </span>apt <span class=\"nb\">install </span>nkf\n<span class=\"o\">{</span> \n  curl <span class=\"nt\">-sSL</span> <span class=\"s2\">\"http://zipcloud.ibsnet.co.jp/zipcodedata/download?di=1625040649647\"</span> <span class=\"nt\">-o</span> ./x_ken_all.zip<span class=\"p\">;</span>\n  unzip <span class=\"nt\">-p</span> x_ken_all.zip | nkf <span class=\"nt\">-w</span><span class=\"p\">;</span>\n  <span class=\"nb\">rm </span>x_ken_all.zip\n<span class=\"o\">}</span> <span class=\"o\">&gt;</span>ken_all.csv\n</code></pre></div></div>\n<p data-sourcepos=\"23:1-23:296\">zipcloudを使うことに抵抗がある場合は<a href=\"https://github.com/oirik/gokenall\" target=\"_blank\" rel=\"noopener noreferrer\">gokenall</a>もありますが、正規化によって一つの郵便番号に複数の町名番地が存在するため郵便番号をキーとするyubinbango-dataに変換する際には工夫が必要です。</p>\n<div class=\"code-block\" data-sourcepos=\"25:1-28:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code>go get github.com/oirik/gokenall/cmd/kenall\n<span class=\"o\">{</span> kenall download <span class=\"nt\">-x</span> | kenall normalize <span class=\"o\">}</span> <span class=\"o\">&gt;</span>ken_all.csv\n</code></pre></div></div>\n<h2 data-sourcepos=\"30:1-30:30\" id=\"2-2-0\" name=\"2-2-0\"><a class=\"anchor\" id=\"jigyosyo.csvを取得する\" name=\"jigyosyo.csvを取得する\" href=\"#jigyosyo.csvを取得する\" data-position=\"2-2-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"jigyosyo.csvを取得する\"> &gt; jigyosyo.csvを取得する</span></a>jigyosyo.csvを取得する</h2>\n<p data-sourcepos=\"31:1-31:54\">jigyosyo.csvは特に正規化は必要ないです。</p>\n<div class=\"code-block\" data-sourcepos=\"33:1-39:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code><span class=\"o\">{</span> \n  curl <span class=\"nt\">-sSL</span> https://www.post.japanpost.jp/zipcode/dl/jigyosyo/zip/jigyosyo.zip <span class=\"nt\">-o</span> ./jigyosyo.zip<span class=\"p\">;</span>\n  unzip <span class=\"nt\">-p</span> jigyosyo.zip | nkf <span class=\"nt\">-w</span><span class=\"p\">;</span>\n  <span class=\"nb\">rm </span>jigyosyo.zip\n<span class=\"o\">}</span> <span class=\"o\">&gt;</span>jigyosyo.csv\n</code></pre></div></div>\n<h2 data-sourcepos=\"41:1-41:33\" id=\"2-3-0\" name=\"2-3-0\"><a class=\"anchor\" id=\"yubinbango-dataを生成する\" name=\"yubinbango-dataを生成する\" href=\"#yubinbango-dataを生成する\" data-position=\"2-3-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"yubinbango-dataを生成する\"> &gt; yubinbango-dataを生成する</span></a>yubinbango-dataを生成する</h2>\n<p data-sourcepos=\"42:1-42:288\">ken_all.csvとjigyosyo.csvをUNIONしてjqで郵便番号をキーとしたオブジェクトに変換します。一部buildingカラムがnullを持っておりyubinbango-dataと異なる部分はありますが、大凡同等の状態にまで持っていくことが出来ました。</p>\n<div class=\"code-block\" data-sourcepos=\"44:1-65:3\"><div class=\"code-filename\"><i class=\"fa fa-file-code-o\"></i>sh</div><div class=\"highlight\"><pre class=\"highlight shell\"><code>brew <span class=\"nb\">install </span>noborus/tap/trdsql\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>001..999<span class=\"o\">}</span><span class=\"p\">;</span> <span class=\"k\">do\n  </span>trdsql <span class=\"nt\">-ojson</span> <span class=\"s2\">\"\n    SELECT *\n    FROM (\n      SELECT c3 zip, c8 city, c9 town, NULL building FROM ken_all.csv\n      UNION SELECT c8 zip, c5 city, c6 town, c7 building FROM jigyosyo.csv\n    )\n    WHERE SUBSTRING(zip,0,4) = '</span><span class=\"nv\">$i</span><span class=\"s2\">'\n    ORDER BY zip ASC\n  \"</span> <span class=\"se\">\\</span>\n  | jq <span class=\"nt\">--compact-output</span> <span class=\"s1\">'\n    .\n    | to_entries\n    | map({ (.value.zip): [1, .value.city, .value.town, .value.building] })\n    | add\n  '</span> <span class=\"se\">\\</span>\n  | <span class=\"nb\">sed</span> <span class=\"nt\">-E</span> <span class=\"s1\">'s/(.+?)/$yubin(\\1);/g'</span> <span class=\"se\">\\</span>\n  <span class=\"o\">&gt;</span><span class=\"nv\">$i</span>.js<span class=\"p\">;</span>\n<span class=\"k\">done</span>\n</code></pre></div></div>\n<h1 data-sourcepos=\"67:1-67:8\" id=\"3-0-0\" name=\"3-0-0\"><a class=\"anchor\" id=\"WRAPUP\" name=\"WRAPUP\" href=\"#WRAPUP\" data-position=\"3-0-0\"><i class=\"fa fa-link\"></i><span class=\"hidden\" data-text=\"WRAPUP\"> &gt; WRAPUP</span></a>WRAPUP</h1>\n<p data-sourcepos=\"68:1-68:208\">昔から何かと話題に上がるken_all.csvですが、正規化のサービスに加えCSV用SQLクライアントとjqの登場により思った以上に簡単に変換することができました。</p>\n","tags":["yubinbango","ken_all.csv","jq","trdsql"],"updated_at":"2021-07-25T12:07:20+09:00","childPublishedDate":{"published_on":"2021-07-25T00:00:00.000Z"},"updated_by":{"name":"なびの👷","screen_name":"nabinno","icon":"https://img.esa.io/uploads/production/members/94286/icon/thumb_m_ef5f024307008aa399b91f87fa5f64e8.jpg"}},"relatedPosts":{"edges":[{"node":{"number":67,"relative_category":"blog/frontend","fields":{"title":"esaをHeadless CMSとして使う","excerpt":"最近仕事の同僚からHeadless CMS という言葉を聞いていて「自分には関係ないな」と距離を取っていたのですが、なぜか回り回って自分からHeadless CMSを作ることになりました。世の中何が起きるか分からないですね。  > PROBLEMPROBLEM \n\n- ブログを普段書かない人なのだが、よそ向けに情報発信する必要が出てきた とは言っても、今までMarkdownをJekyllで管理していたので画像を貼り付けるのが手間でモチベーションが大きく下がっていた さらにPlantUMLを出力するのも手間、試行錯誤した末にいずれも付け焼き刃で、esaの操作感に勝てるものはなかった \n- とは言っても、今までMarkdownをJekyllで管理していたので画像を貼り付けるのが手間でモチベーションが大きく下がっていた さらにPlantUMLを出力するのも手間、試行錯誤した末にいずれも付け焼き刃で、esaの操作感に勝てるものはなかった \n- さらにPlantUMLを出力するのも手間、試行錯誤した末にいずれも付け焼き刃で、esaの操作感に勝てるものはなかった  > SOLUTIONSOLUTION \n\nというわけで、esaをHeadless CMSとして使うことにしました。 \n\nやってることは昔のMovableTypeそのもので懐かしかったです。コンテンツを別システムで管理しビルドサーバーに当該コンテンツを流し込みリビルド、最後にホストサーバーにアップロードというワークフロー。今はJAMStackの文脈で語られているようです。 \n\nこのHeadless CMSが昔と違うのはコンテンツ作成に集中できること。CI周りが発達したので一度ワークフローを組み立てれば後は自動でコンテンツを生成できます。  > やり方やり方 \n\n- esa.io でゆるふわ情報共有 - Middleman Blog への Export サンプル付き #esa_io - Qiita\n- 技術ブログを支える技術（Gatsby + esaio） - mottox2 blog\n- Next.jsとesaを使った個人サイト構築 | corocn.dev \n\nそれほど時間をかけられなかったので、上記3記事の中で手軽さを考慮しmottox2さんのソースコードを拝借しました。ありがとうございます。 \n\n- 作ったレポジトリ：nabinno/nabinno.github.io: On Blahfe - Nab's Github Pages  > シークエンス図シークエンス図 \n\n私が手を入れたのはコンポーネントを削りGatsby Blog Starterに寄せたのと、デプロイ方法を使い慣れたCircleCIに変えたくらいです。 \n\nGitHub PagesにはVercelのような便利なWebhookがないので、esaで実装されたGitHub Webhook連携を使いそれをトリガーにCircleCIジョブを走らせています。 \n\n  > CircleCIジョブCircleCIジョブ \n\nまた、CircleCIジョブは何の変哲もないもので、NodeJSを叩いてGitプッシュしているくらいです。先ほどのGitHub Webhookと似た感じの泥臭いワークフローは [skip ci] コメントの追加があります。当該コメントを入れないとジョブが再帰的に走り続けるので出口で明示してあります。 yml\n\nversion: 2.1 jobs: build_deploy: docker: - image: circleci/node:12.4 steps: - checkout - run: name: Install NPM command: npm install - run: name: Build command: npm run clean && npm run build - add_ssh_keys: fingerprints: - \"{foo}\" - deploy: name: Deploy command: | git config --global user.email \"nab+circleci@blahfe.com\" git config --global user.name \"nabinno+circleci\" git add . git commit -m \"[skip ci]Run npm run clean && npm run build.\" git push origin master workflows: build_deploy: jobs: - build_deploy: filters: branches: only: master   > WRAPUPWRAPUP \n\nとまあ大した作業内容ではないのですが、久しぶりに昔懐かしのMovableTypeのリビルドを思い出しつつ、副産物として全く縁遠かったNetlifyとVercelの位置づけを薄らと感じ取れました。"},"name":"[2021-01-18]esaをHeadless CMSとして使う","tags":["gatsby","esa","headless-cms","cms"],"childPublishedDate":{"published_on":"2021-01-18T00:00:00.000Z","published_on_unix":1610928000}}},{"node":{"number":44,"relative_category":"blog/organization","fields":{"title":"整理したい私はITILをかぶる、PlantUMLへの愛","excerpt":"現在ネクイノでエンジニアリングマネージャー、バックエンドエンジニア、インフラエンジニアを担当しています。入社後8ヶ月、年の瀬ということで振り返り記事を書くことにしました。テーマを一つに絞らないと記事にならないので今回はPlantUMLに絞ります。断りとして、この記事で書いてあることはITILプラクティスを一部なぞっているに過ぎません。PlantUMLが全知全能のツールということを主張したいわけではないです、ただ愛しています。   > PROBLEMPROBLEM \n\n- 開発人数が増えるにあたり、チームとして機能していない 管理規程はあるものの 業務フローが明示化されておらず、誰が何を何の目的で業務を回しているか分からない 可視化されていないプロセスが問題になるケースが増えてきた \n- 管理規程はあるものの 業務フローが明示化されておらず、誰が何を何の目的で業務を回しているか分からない 可視化されていないプロセスが問題になるケースが増えてきた \n- 可視化されていないプロセスが問題になるケースが増えてきた  > SOLUTIONSOLUTION \n\nと言うわけで、入社早々PlantUMLで業務フローを可視化することを始めました。  > PlantUMLとはPlantUMLとは \n\nPlantUMLはオープンソースのUMLダイアグラム作成用のテキストベースの言語です。シークエンス図、ユースケース図、アクティビティ図、クラス図のようなダイアグラムをシンプルで直感的に書くことができます。 \n\n2009年リリースされており、私が使うようになったのは、Emacsのorg-babelで実装されてからなので2014年くらい1。2016-7年にesa.ioやVS Code等で実装されてから爆発的に普及したと記憶しています。「esa.ioはオンラインのorg-modeになるべくPlantUMLを実装すべき」と要望したのは良い思い出です。  > やったことやったこと \n\nさて、私はネクイノに入社早々既存システムの運用開発と情シス（業務運用）の部長職にアサインされました。既存システムの運用開発は新しく外部のパートナーが入ると言うことで、開発フローが大きく変わる節目にありました。  > 開発フローを整備する開発フローを整備する \n\n話を聞くに新しく入る外部パートナーはプロジェクトマネージャ、ブリッジエンジニア・コミュニケーター、モバイルエンジニア、バックエンドエンジニア、フロントエンジニア、品質チェック含め20名程の体制でした。また、既存システムの運用開発ではプロダクトマネジャー、プロダクトオーナーが各開発者とともに企画策定を行うことが慣習として存在していました。私はまず企画から実装、レビュー、リリースまでの流れを整理します。 Jira上の大まかな流れ \n\n 開発の流れ \n\n  > 要望フローを整備する要望フローを整備する \n\n次に、機能要望、バグ報告、改善要望がSlackチャンネルの至る所に散在している上、チケット化されないケースがありました。突貫ではありますが、GoogleフォームとJira連携を行いました2。 \n\nプロダクトマネジャーの体制が整備されてからは、機能要望のフォームは使われることはなくなりましたが、バグ報告、改善要望は要所要所で使われ、トリアージという形で定期的に活用されています。 \n\n  > デプロイフローを整備するデプロイフローを整備する \n\n開発が進んでいくと、今度は開発環境が足りなくなりました。当時はステージング環境と本番環境しかなく、かつ、ステージング環境がテスト環境兼デモ環境の役割を呈しており、ステージング環境おテストで不具合を起こすとデモに影響が出るという状態が続いておりました。また、外部パートナーが開発するに当たり繊細なステージング環境を使うのが難しいため進捗に影響が出始めておりました。 \n\n急を要する事態のためAWS CDKでステージング環境とは別に結合環境を用意し3、デプロイフローを整備しました。 \n\n  > 障害対応フローを整備する障害対応フローを整備する \n\nさて、運用開発が順調に進んでいくと、今度は障害が頻繁に起きていることに気づきました。いいえ、薄々気づいていたのですが多忙にかまけて蓋をしておりました。ここに関しては本腰を入れてAWSサポートプランをビジネスに変更し原因を突き止めました。協力いただいた各位には感謝です。 \n\nまた、今まで見過ごされていたGoogle Workspace等の業務運用のシステムも含め障害報告の体制を敷くとともに、監視体制も強化しました。 \n\n  > 業務フローを整理する業務フローを整理する \n\nまだまだあります。業務内容に関しては詳細は書けませんが、部内の業務から他部署の業務まで安全に生産性を高めるため整理を行いました。まだまだ行います。  > リモート飲みのフローを整備するリモート飲みのフローを整備する \n\nいよいよ疲れてきたのでお酒が飲みたくなりました。飲み会フローを作ってみましたが思いの外手間がかかることが分かりあまり活用できておりません。その代わり社内でオンラインシャッフルランチという制度ができました。 \n\n  > 分かったこと分かったこと \n\nはい、こうして振り返ると入社時に感じていた雑然さは業務フローが明確でない状態のことでした。開発者なら分かると思いますが、企画段階で思い描く構成図は実装する段になるとあまり意味をなさず、結局は頭の中はシークエンス図でいっぱいになります。それと同じで、登場人物、登場人物間のメッセージ、メッセージの大枠が関係者に共有されていないと、いくらリソースが投下されても不安定で生産性に伸び悩むのです。つまり、雑然とした環境を整理すると言うことはシークエンス図を書くことに他なりません。 \n\nしかしながら、当該環境一つ一つを俯瞰的に見るとITILプラクティスそのものであることにも気づきます。 \n\nITILとはITサービスマネジメントのベストプラクティスフレームワークのこと。何らかの高い技術を持っていても、投資対効果を考えていなければ赤字になりビジネスと成り立ちませんし、顧客のことを考えずに作ったものに価値はありませんし、サービスの評価を落とすことになります。このようなことを防ぐには顧客目線やビジネス的な観点が必要で、そのノウハウがまとまったものがITILです。  > 今回対応したプラクティス今回対応したプラクティス \n\n今回の振り返りでは具体的に次のプラクティスをなぞっておりました。    振り返り ITILプラクティス     開発フローを整備する 継続的サービス改善   要望フローを整備する 要求管理、問題管理   デプロイフローを整備する リリース管理及び展開管理   障害対応フローを整備する インシデント管理   業務フローを整理する CMMI   リモート飲みのフローを整備する 組織変更管理    \n\nCMMIと組織変更管理が分かりづらいの少し補足します。 \n\n- CMMIとは能力成熟度モデル統合のことで、業務フローを評価し5段階で成熟度レベルを出す手法です。現状はレベル1-2（初期段階）のものがほとんどなのでまずはPlantUMLを使い共通認識を作るところから始めました。\n- 組織変更管理とは経営学で言うところのチェンジマネジメントに当たります。ここでは各種フローを整備しメンバー全員に落とし込むことを目指します。『Fearless Change』では今回のリモート飲み以外にも多くのパターンランゲージが紹介されています。  > WRAPUPWRAPUP  > 次にすること次にすること \n\nネクストアクションですが、採用フローを考えています。 \n\n（読者の皆様はどんなシークエンス図を思い浮かべましたか?） \n\nというわけで、ネクイノはPlantUMLを愛している開発者を募集中です。  > PR__colon__ ネクイノとはPR: ネクイノとは \n\n「世界中の医療空間と体験を再定義する」をミッションに、人々と医療の間にICTのチカラで橋をかける遠隔医療ソリューションを手掛けている会社です。医療というと高齢の患者さんをイメージされるかもしれませんが、我らがターゲットとしているのは現役世代の方。病気を治療するというより、現役世代がQOLを高めるためのサポートを目的としています。 \n\nメインサービスは、女性に特化したピルのオンライン診療アプリ「スマルナ」。ピルを飲まれている人だけでなく、受診や服用に抵抗がある方にも気軽に利用していただけたらと思いサービス提供しています。診察室の手前に助産師と薬剤師を配置した相談室を設ける等、受診のハードルを下げる工夫をそこかしこに施しているのが特徴です。 \n\n様々なメディカルコミュニケーションを行っています - 専門家相談 - カスタマーサポート - ユーザーコミュニティ  \n\n妻からは「10年前にサービスがあったら良かったのに」とお墨付きをいただいており、興味をもった方は詳しくはこちらをご覧下さい。 https://smaluna.com/  \n\n1. [B! plantuml] nabinnoのブックマーク ↩ \n2. https://github.com/nabinno/google-forms-to-jira-slack ↩ \n3. CDKはaws-rails-provisionerを参考に ecs_patterns.ApplicationLoadBalancedFargateService を実装しました ↩"},"name":"[2020-12-30]整理したい私はITILをかぶる、PlantUMLへの愛","tags":["team-building"],"childPublishedDate":{"published_on":"2020-12-30T00:00:00.000Z","published_on_unix":1609286400}}},{"node":{"number":124,"relative_category":"blog/backend","fields":{"title":"Increment Pは住所のバリデーションチェックでどの程度使えるか","excerpt":"7月に調査した「imi-enrichment-addressは住所のバリデーションチェックでどの程度使えるか」の続きになります。コロナ禍であらゆる流通がオンラインに移行する中、正しい住所を使うことはいっそう求められています。ユーザーが配送用に住所を入力する時そのデータが正しいとどうやって判定するのでしょうか。今回は商用サービスIncrement Pが住所のバリデーションチェックでどの程度使えるか検証してみました。   > PROBLEMPROBLEM \n\n- 住所の不備が至るところで起きている 特に町名番地の抜けもれや不備が多くこの点をどうにか拾いたい 可能ならユーザーの入力時点でFEあるいはBE側でバリデーションチェックしたい imi-enrichment-addressで精度が思わしくなかったので今回は商用サービスで検証したい \n- 特に町名番地の抜けもれや不備が多くこの点をどうにか拾いたい\n- 可能ならユーザーの入力時点でFEあるいはBE側でバリデーションチェックしたい imi-enrichment-addressで精度が思わしくなかったので今回は商用サービスで検証したい \n- imi-enrichment-addressで精度が思わしくなかったので今回は商用サービスで検証したい  > SOLUTIONSOLUTION \n\nというわけで、住所のバリデーションチェックで商用版「Increment P」がどの程度使えるか検証します。  > Increment PとはIncrement Pとは \n\n住所をAPIを介すことで正規化することができます。APIの返値に解析レベル・解析ログを加えることでより柔軟な検証をおこなうことができるようになっています。 \n\n解析レベルとは、対象住所のマッチ度合いを都道府県・市区町村・町域・丁目・番地・号というレベルで分けたものです。APIの結果が解析レベル5「番地・番」以上になっていれば配送が確実に為されると言うように、配送の確実性を前提にして住所の入力者とやりとりを実現します。また、解析ログメッセージとは、住所の正規化を試みた際のログであり、バリデーションを調整する際に頻繁に確認するものです。詳細は「ドキュメント」をご覧下さい。    解析レベル レベルの数字 説明     都道府県 1 県レベルでマッチしました   市区町村 2 市区町村レベルでマッチしました   町域 (大字) 3 町域レベルでマッチしました   丁目 / 小字 4 丁目または小字レベルでマッチしました   番地（番） 5 番地（番）レベルでマッチしました   号情報が存在しない番地 7 番地（番）レベルでマッチしました（号情報が存在しない地域）   号 8 号レベルでマッチしました   不明 -1 不明    \n\n試しにIncrement Pを実行してみましょう。正確な住所を渡したときと不正確な住所を渡したときで解析レベルが5と3と異なった結果を返すことが見て取れます。 sh\n\n$ curl \"https://api-anorm.mapfan.com/v1/$(echo -n 長野県長野市大字長野旭町1108 | jq -sRr @uri).json\" \\ -H 'x-api-key: <api-key>' \\ -H 'Content-Type: application/json' | jq -r { \"type\": \"FeatureCollection\", \"query\": [ \"長野県長野市大字長野旭町1108\" ], \"features\": [ { \"type\": \"Feature\", \"geometry\": null, \"properties\": { \"query\": \"長野県長野市大字長野旭町1108\", \"place_name\": \"長野県長野市長野旭町 1108\", \"pref\": \"長野県\", \"pref_kana\": \"ナガノケン\", \"city\": \"長野市\", \"city_kana\": \"ナガノシ\", \"area\": \"長野\", \"area_kana\": \"ナガノ\", \"koaza_chome\": \"旭町\", \"koaza_chome_kana\": \"アサヒマチ\", \"banchi_go\": \"1108\", \"building\": \"\", \"building_number\": \"\", \"zipcode\": \"3800846\", \"geocoding_level\": 5, \"geocoding_level_desc\": \"番地（番）レベルでマッチしました(5)\", \"log\": \"RM002:[大字(字)]の文字を除去しました\", \"not_normalized\": \"\" } } ], \"attribution\": \"(c) INCREMENT P CORPORATION\" } $ curl \"https://api-anorm.mapfan.com/v1/$(echo -n 長野県長野市旭町1108 | jq -sRr @uri).json\" \\ -H 'x-api-key: <api-key>' \\ -H 'Content-Type: application/json' | jq -r { \"type\": \"FeatureCollection\", \"query\": [ \"長野県長野市旭町1108\" ], \"features\": [ { \"type\": \"Feature\", \"geometry\": null, \"properties\": { \"query\": \"長野県長野市旭町1108\", \"place_name\": \"長野県長野市旭町\", \"pref\": \"長野県\", \"pref_kana\": \"ナガノケン\", \"city\": \"長野市\", \"city_kana\": \"ナガノシ\", \"area\": \"旭町\", \"area_kana\": \"アサヒマチ\", \"koaza_chome\": \"\", \"koaza_chome_kana\": \"\", \"banchi_go\": \"\", \"building\": \"\", \"building_number\": \"\", \"zipcode\": \"3800846\", \"geocoding_level\": 3, \"geocoding_level_desc\": \"町域レベルでマッチしました(3)\", \"log\": \"NT001:正規化処理状況が建物名正規化処理の必要条件を満たさないので建物名正規化は行われません\", \"not_normalized\": \"1108\" } } ], \"attribution\": \"(c) INCREMENT P CORPORATION\" }  \n\nなお、上記結果を見て分かるとおり、Increment Pは大字省略には強そうですが町域自体の省略は苦手なようです。imi-enrichment-addressより柔軟ですが、基本は街区レベル位置参照情報を利用しているように推察されます。  > 検証用データ検証用データ \n\nさて、検証用データですが、imi-enrichment-addressの検証データと合わせて住所.jpを使います。今回はトライアルが1000件と制限があるので、imi-enrichment-addressで無効割合が54.42%と一番多かった青森県と住所の登録数が多い東京・愛知・北海道・大阪・福岡・神奈川、さらに通りが独特な京都、町字の組み合わせで住所が2つ以上存在する長野に対象を絞ります。各々100件ずつの検証になります。 sh\n\n$ { curl -sSL http://jusyo.jp/downloads/new/csv/csv_zenkoku.zip -o csv_zenkoku.zip; unzip -p csv_zenkoku.zip | nkf -w; rm csv_zenkoku.zip } >zenkoku.csv $ brew install noborus/tap/trdsql $ trdsql \" SELECT COUNT(*) FROM zenkoku.csv WHERE c21 <> '' \" 22431 $ trdsql -otbln \" SELECT c8, count(*) cn FROM zenkoku.csv WHERE c21 != '' GROUP BY c8 ORDER BY cn DESC\" | 都道府県 | count(*) | | --- | --- | | 東京都 | 4734 | | 愛知県 | 1541 | | 北海道 | 1251 | | 大阪府 | 884 | | 福岡県 | 845 | | 神奈川県 | 820 | [..] | 長野県 | 594 | [..] | 京都府 | 255 | [..] | 青森県 | 216 |   > Increment Pで検証用データを確認するIncrement Pで検証用データを確認する sh\n\n$ for p in 東京都 愛知県 北海道 大阪府 福岡県 神奈川県 青森県 京都府 長野県; do for a in $(trdsql \" SELECT c8||c10||c21 FROM zenkoku.csv WHERE c21 != '' AND c8 = '$p' ORDER BY RANDOM() LIMIT 100 \"); do curl -w'\\n' \"https://api-anorm.mapfan.com/v1/$(echo -n $a | jq -sRr @uri).json\" \\ -H 'x-api-key: <api-key>' \\ -H 'Content-Type: application/json' >>output.jsonl; done & done &   > 解析結果を確認する解析結果を確認する \n\nIncrement Pの解析結果を確認したところ、imi-enrichment-addressと比べると大方改善しました。特に青森県、北海道の改善率は高く字・条・線に対して有効に機能していることが伺えます。一方、京都や長野のように特殊な住所がある府県については改善が思うように行かないケースもあるようです。 sh\n\n$ cat output.jsonl \\ | jq -r '[ .features[].properties.pref, .features[].properties.query, .features[].properties.geocoding_level, .features[].properties.log ] | @csv' \\ | trdsql -otbln \" SELECT c1, COUNT(*) cn FROM - WHERE c3 >= 5 GROUP BY c1 ORDER BY cn DESC \"  \n\n解析レベル5「番地・番」以上の場合（※ 参考値はimi-enrichment-addressの有効割合）    都道府県 有効割合 参考値     東京都 100 99.11   大阪府 100 96.72   福岡県 95 91   神奈川県 95 98.28   愛知県 92 92.6   青森県 90 45.58   長野県 84 55.72   北海道 80 86.24   京都府 79 63.14    \n\n解析レベル4「丁目/小字」以上の場合（※ 参考値はimi-enrichment-addressの有効割合）    都道府県 有効割合 参考値     東京都 100 99.11   大阪府 100 96.72   北海道 98 86.24   愛知県 97 92.6   福岡県 96 91   神奈川県 95 98.28   青森県 93 45.58   長野県 84 55.72   京都府 79 63.14     > WRAPUPWRAPUP \n\n青森県の有効率が45.58%だったimi-enrichment-addressと比べると、Increment Pは調査した大凡の都道府県で改善し70%以上の有効割合を出していました。バリデーションチェックで使えるのかというと全ての都道府県で100%になっていないため心許ない状況ではあるものの、解析レベル4「丁目/小字」以下の住所については最終確認を促すフローを入れる等ひと手間加えれば実用に耐えうると考えます。"},"name":"[2021-11-23]Increment Pは住所のバリデーションチェックでどの程度使えるか","tags":["incrementp"],"childPublishedDate":{"published_on":"2021-11-23T00:00:00.000Z","published_on_unix":1637625600}}}]}},"pageContext":{"number":90}},"staticQueryHashes":[]}